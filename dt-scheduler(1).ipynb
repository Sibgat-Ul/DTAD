{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:34:40.272009Z",
     "iopub.status.busy": "2025-01-14T14:34:40.271810Z",
     "iopub.status.idle": "2025-01-14T14:34:57.598296Z",
     "shell.execute_reply": "2025-01-14T14:34:57.597196Z",
     "shell.execute_reply.started": "2025-01-14T14:34:40.271981Z"
    },
    "papermill": {
     "duration": 4.486292,
     "end_time": "2024-12-28T15:03:24.743169",
     "exception": false,
     "start_time": "2024-12-28T15:03:20.256877",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorboard tensorboardX -q\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import getpass\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from __future__ import absolute_import\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, resnet101, resnet34\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:34:57.600554Z",
     "iopub.status.busy": "2025-01-14T14:34:57.599957Z",
     "iopub.status.idle": "2025-01-14T14:34:58.049389Z",
     "shell.execute_reply": "2025-01-14T14:34:58.048374Z",
     "shell.execute_reply.started": "2025-01-14T14:34:57.600518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dtkd_losses = []\n",
    "dtkd_accuracies = []\n",
    "our_losses = []\n",
    "our_accuracies = []\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes,\n",
    "                    self.expansion * planes,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion * planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            planes, self.expansion * planes, kernel_size=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes,\n",
    "                    self.expansion * planes,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion * planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "        self.stage_channels = [256, 512, 1024, 2048]\n",
    "\n",
    "    def get_feat_modules(self):\n",
    "        feat_m = nn.ModuleList([])\n",
    "        feat_m.append(self.conv1)\n",
    "        feat_m.append(self.bn1)\n",
    "        feat_m.append(self.layer1)\n",
    "        feat_m.append(self.layer2)\n",
    "        feat_m.append(self.layer3)\n",
    "        feat_m.append(self.layer4)\n",
    "        return feat_m\n",
    "\n",
    "    def get_bn_before_relu(self):\n",
    "        if isinstance(self.layer1[0], Bottleneck):\n",
    "            bn1 = self.layer1[-1].bn3\n",
    "            bn2 = self.layer2[-1].bn3\n",
    "            bn3 = self.layer3[-1].bn3\n",
    "            bn4 = self.layer4[-1].bn3\n",
    "        elif isinstance(self.layer1[0], BasicBlock):\n",
    "            bn1 = self.layer1[-1].bn2\n",
    "            bn2 = self.layer2[-1].bn2\n",
    "            bn3 = self.layer3[-1].bn2\n",
    "            bn4 = self.layer4[-1].bn2\n",
    "        else:\n",
    "            raise NotImplementedError(\"ResNet unknown block error !!!\")\n",
    "\n",
    "        return [bn1, bn2, bn3, bn4]\n",
    "\n",
    "    def get_stage_channels(self):\n",
    "        return self.stage_channels\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = strides[i]\n",
    "            layers.append(block(self.in_planes, planes, stride, i == num_blocks - 1))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def encode(self, x, idx, preact=False):\n",
    "        if idx == -1:\n",
    "            out, pre = self.layer4(F.relu(x))\n",
    "        elif idx == -2:\n",
    "            out, pre = self.layer3(F.relu(x))\n",
    "        elif idx == -3:\n",
    "            out, pre = self.layer2(F.relu(x))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        return pre\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        f0 = out\n",
    "        out, f1_pre = self.layer1(out)\n",
    "        f1 = out\n",
    "        out, f2_pre = self.layer2(out)\n",
    "        f2 = out\n",
    "        out, f3_pre = self.layer3(out)\n",
    "        f3 = out\n",
    "        out, f4_pre = self.layer4(out)\n",
    "        f4 = out\n",
    "        out = self.avgpool(out)\n",
    "        avg = out.reshape(out.size(0), -1)\n",
    "        out = self.linear(avg)\n",
    "\n",
    "        feats = {}\n",
    "        feats[\"feats\"] = [f0, f1, f2, f3, f4]\n",
    "        feats[\"preact_feats\"] = [f0, f1_pre, f2_pre, f3_pre, f4_pre]\n",
    "        feats[\"pooled_feat\"] = avg\n",
    "\n",
    "        return out, feats\n",
    "\n",
    "\n",
    "def ResNet18(**kwargs):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "\n",
    "def ResNet34(**kwargs):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def ResNet50(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def ResNet101(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "\n",
    "def ResNet152(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    net = ResNet18(num_classes=100)\n",
    "    x = torch.randn(2, 3, 32, 32)\n",
    "    logit, feats = net(x)\n",
    "\n",
    "    for f in feats[\"feats\"]:\n",
    "        print(f.shape, f.min().item())\n",
    "    print(logit.shape)\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, depth, num_filters, block_name=\"BasicBlock\", num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        # Model type specifies number of layers for CIFAR-10 model\n",
    "        if block_name.lower() == \"basicblock\":\n",
    "            assert (\n",
    "                depth - 2\n",
    "            ) % 6 == 0, \"When use basicblock, depth should be 6n+2, e.g. 20, 32, 44, 56, 110, 1202\"\n",
    "            n = (depth - 2) // 6\n",
    "            block = BasicBlock\n",
    "        elif block_name.lower() == \"bottleneck\":\n",
    "            assert (\n",
    "                depth - 2\n",
    "            ) % 9 == 0, \"When use bottleneck, depth should be 9n+2, e.g. 20, 29, 47, 56, 110, 1199\"\n",
    "            n = (depth - 2) // 9\n",
    "            block = Bottleneck\n",
    "        else:\n",
    "            raise ValueError(\"block_name shoule be Basicblock or Bottleneck\")\n",
    "\n",
    "        self.inplanes = num_filters[0]\n",
    "        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters[0])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, num_filters[1], n)\n",
    "        self.layer2 = self._make_layer(block, num_filters[2], n, stride=2)\n",
    "        self.layer3 = self._make_layer(block, num_filters[3], n, stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(num_filters[3] * block.expansion, num_classes)\n",
    "        self.stage_channels = num_filters\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.inplanes,\n",
    "                    planes * block.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = list([])\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, downsample, is_last=(blocks == 1))\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, is_last=(i == blocks - 1)))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_feat_modules(self):\n",
    "        feat_m = nn.ModuleList([])\n",
    "        feat_m.append(self.conv1)\n",
    "        feat_m.append(self.bn1)\n",
    "        feat_m.append(self.relu)\n",
    "        feat_m.append(self.layer1)\n",
    "        feat_m.append(self.layer2)\n",
    "        feat_m.append(self.layer3)\n",
    "        return feat_m\n",
    "\n",
    "    def get_bn_before_relu(self):\n",
    "        if isinstance(self.layer1[0], Bottleneck):\n",
    "            bn1 = self.layer1[-1].bn3\n",
    "            bn2 = self.layer2[-1].bn3\n",
    "            bn3 = self.layer3[-1].bn3\n",
    "        elif isinstance(self.layer1[0], BasicBlock):\n",
    "            bn1 = self.layer1[-1].bn2\n",
    "            bn2 = self.layer2[-1].bn2\n",
    "            bn3 = self.layer3[-1].bn2\n",
    "        else:\n",
    "            raise NotImplementedError(\"ResNet unknown block error !!!\")\n",
    "\n",
    "        return [bn1, bn2, bn3]\n",
    "\n",
    "    def get_stage_channels(self):\n",
    "        return self.stage_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)  # 32x32\n",
    "        f0 = x\n",
    "\n",
    "        x, f1_pre = self.layer1(x)  # 32x32\n",
    "        f1 = x\n",
    "        x, f2_pre = self.layer2(x)  # 16x16\n",
    "        f2 = x\n",
    "        x, f3_pre = self.layer3(x)  # 8x8\n",
    "        f3 = x\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        avg = x.reshape(x.size(0), -1)\n",
    "        out = self.fc(avg)\n",
    "\n",
    "        feats = {}\n",
    "        feats[\"feats\"] = [f0, f1, f2, f3]\n",
    "        feats[\"preact_feats\"] = [f0, f1_pre, f2_pre, f3_pre]\n",
    "        feats[\"pooled_feat\"] = avg\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet8(**kwargs):\n",
    "    return ResNet(8, [16, 16, 32, 64], \"basicblock\", **kwargs)\n",
    "\n",
    "\n",
    "def resnet14(**kwargs):\n",
    "    return ResNet(14, [16, 16, 32, 64], \"basicblock\", **kwargs)\n",
    "\n",
    "\n",
    "def resnet20(**kwargs):\n",
    "    return ResNet(20, [16, 16, 32, 64], \"basicblock\", **kwargs)\n",
    "\n",
    "\n",
    "def resnet32(**kwargs):\n",
    "    return ResNet(32, [16, 16, 32, 64], \"basicblock\", **kwargs)\n",
    "\n",
    "\n",
    "def resnet44(**kwargs):\n",
    "    return ResNet(44, [16, 16, 32, 64], \"basicblock\", **kwargs)\n",
    "\n",
    "\n",
    "def resnet56(**kwargs):\n",
    "    return ResNet(56, [16, 16, 32, 64], \"basicblock\", **kwargs)\n",
    "\n",
    "\n",
    "def resnet110(**kwargs):\n",
    "    return ResNet(110, [16, 16, 32, 64], \"basicblock\", **kwargs)\n",
    "\n",
    "\n",
    "def resnet8x4(**kwargs):\n",
    "    return ResNet(8, [32, 64, 128, 256], \"basicblock\", **kwargs)\n",
    "\n",
    "\n",
    "def resnet32x4(**kwargs):\n",
    "    return ResNet(32, [32, 64, 128, 256], \"basicblock\", **kwargs)\n",
    "\n",
    "cifar100_model_prefix = \"/kaggle/input/cifar_teachers/pytorch/default/1/cifar_teachers/\"\n",
    "\n",
    "cifar_model_dict = {\n",
    "    # teachers\n",
    "    \"resnet56\": (\n",
    "        resnet56,\n",
    "        cifar100_model_prefix + \"resnet56_vanilla/ckpt_epoch_240.pth\",\n",
    "    ),\n",
    "    \"resnet110\": (\n",
    "        resnet110,\n",
    "        cifar100_model_prefix + \"resnet110_vanilla/ckpt_epoch_240.pth\",\n",
    "    ),\n",
    "    \"resnet32x4\": (\n",
    "        resnet32x4,\n",
    "        cifar100_model_prefix + \"resnet32x4_vanilla/ckpt_epoch_240.pth\",\n",
    "    ),\n",
    "    \"ResNet50\": (\n",
    "        ResNet50,\n",
    "        cifar100_model_prefix + \"ResNet50_vanilla/ckpt_epoch_240.pth\",\n",
    "    ),\n",
    "    # \"wrn_40_2\": (\n",
    "    #     wrn_40_2,\n",
    "    #     cifar100_model_prefix + \"wrn_40_2_vanilla/ckpt_epoch_240.pth\",\n",
    "    # ),\n",
    "    # \"vgg13\": (vgg13_bn, cifar100_model_prefix + \"vgg13_vanilla/ckpt_epoch_240.pth\"),\n",
    "    # students\n",
    "    \"resnet8\": (resnet8, None),\n",
    "    \"resnet14\": (resnet14, None),\n",
    "    \"resnet20\": (resnet20, None),\n",
    "    \"resnet32\": (resnet32, None),\n",
    "    \"resnet44\": (resnet44, None),\n",
    "    \"resnet8x4\": (resnet8x4, None),\n",
    "    \"ResNet18\": (ResNet18, None),\n",
    "    # \"wrn_16_1\": (wrn_16_1, None),\n",
    "    # \"wrn_16_2\": (wrn_16_2, None),\n",
    "    # \"wrn_40_1\": (wrn_40_1, None),\n",
    "    # \"vgg8\": (vgg8_bn, None),\n",
    "    # \"vgg11\": (vgg11_bn, None),\n",
    "    # \"vgg16\": (vgg16_bn, None),\n",
    "    # \"vgg19\": (vgg19_bn, None),\n",
    "    # \"MobileNetV2\": (mobile_half, None),\n",
    "    # \"ShuffleV1\": (ShuffleV1, None),\n",
    "    # \"ShuffleV2\": (ShuffleV2, None),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:34:58.051008Z",
     "iopub.status.busy": "2025-01-14T14:34:58.050755Z",
     "iopub.status.idle": "2025-01-14T14:34:58.086547Z",
     "shell.execute_reply": "2025-01-14T14:34:58.085497Z",
     "shell.execute_reply.started": "2025-01-14T14:34:58.050987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Distiller(nn.Module):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        # teacher as eval mode by default\n",
    "        if not isinstance(mode, bool):\n",
    "            raise ValueError(\"training mode is expected to be boolean\")\n",
    "        self.training = mode\n",
    "        for module in self.children():\n",
    "            module.train(mode)\n",
    "        self.teacher.eval()\n",
    "        return self\n",
    "\n",
    "    def get_learnable_parameters(self):\n",
    "        # if the method introduces extra parameters, re-impl this function\n",
    "        return [v for k, v in self.student.named_parameters()]\n",
    "\n",
    "    def get_extra_parameters(self):\n",
    "        # calculate the extra parameters introduced by the distiller\n",
    "        return 0\n",
    "\n",
    "    def forward_train(self, **kwargs):\n",
    "        # training function for the distillation method\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward_test(self, image):\n",
    "        return self.student(image)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        if self.training:\n",
    "            return self.forward_train(**kwargs)\n",
    "        return self.forward_test(kwargs[\"image\"])\n",
    "\n",
    "class DTKD(Distiller):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(DTKD, self).__init__(student, teacher)\n",
    "        self.ce_loss_weight = 1.0\n",
    "        self.alpha = 3.0\n",
    "        self.beta = 1.0\n",
    "        self.warmup = 20\n",
    "        self.temperature = 4\n",
    "\n",
    "    def forward_train(self, image, target, **kwargs):\n",
    "        logits_student = self.student(image)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits_teacher = self.teacher(image)\n",
    "\n",
    "        # DTKD Loss\n",
    "        reference_temp = self.temperature\n",
    "        logits_student_max, _ = logits_student.max(dim=1, keepdim=True)\n",
    "        logits_teacher_max, _ = logits_teacher.max(dim=1, keepdim=True)\n",
    "        logits_student_temp = 2 * logits_student_max / (logits_teacher_max + logits_student_max) * reference_temp \n",
    "        logits_teacher_temp = 2 * logits_teacher_max / (logits_teacher_max + logits_student_max) * reference_temp\n",
    "        \n",
    "        ourskd = nn.KLDivLoss(reduction='none')(\n",
    "            F.log_softmax(logits_student / logits_student_temp, dim=1) , # 学生\n",
    "            F.softmax(logits_teacher / logits_teacher_temp, dim=1)       # 老师\n",
    "        ) \n",
    "        loss_ourskd = (ourskd.sum(1, keepdim=True) * logits_teacher_temp * logits_student_temp).mean()\n",
    "        \n",
    "        # Vanilla KD Loss\n",
    "        vanilla_temp = self.temperature\n",
    "        kd = nn.KLDivLoss(reduction='none')(\n",
    "            F.log_softmax(logits_student / vanilla_temp, dim=1) , # 学生\n",
    "            F.softmax(logits_teacher / vanilla_temp, dim=1)       # 老师\n",
    "        ) \n",
    "        loss_kd = (kd.sum(1, keepdim=True) * vanilla_temp ** 2).mean() \n",
    "         \n",
    "        # CrossEntropy Loss\n",
    "        loss_ce = nn.CrossEntropyLoss()(logits_student, target)\n",
    "\n",
    "        loss_dtkd = min(kwargs[\"epoch\"] / self.warmup, 1.0) * (self.alpha * loss_ourskd + self.beta * loss_kd) + self.ce_loss_weight * loss_ce\n",
    "        losses_dict = {\n",
    "            \"loss_dtkd\": loss_dtkd,\n",
    "        }\n",
    "\n",
    "        return logits_student, losses_dict\n",
    "\n",
    "class BaseTrainer(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        experiment_name, \n",
    "        distiller, \n",
    "        train_loader, \n",
    "        val_loader\n",
    "    ):\n",
    "        self.distiller = distiller\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.distiller.get_learnable_parameters(), \n",
    "            lr=0.05, \n",
    "            weight_decay=1.5e-5\n",
    "        )\n",
    "        self.best_acc = -1\n",
    "\n",
    "        username = getpass.getuser()\n",
    "        # init loggers\n",
    "        self.log_path = os.path.join(\"./output\", experiment_name)\n",
    "        if not os.path.exists(self.log_path):\n",
    "            os.makedirs(self.log_path)\n",
    "        self.tf_writer = SummaryWriter(os.path.join(self.log_path, \"train.events\"))\n",
    "\n",
    "    def adjust_learning_rate(self, epoch, optimizer):\n",
    "        steps = np.sum(epoch > np.asarray([62, 75, 87]))\n",
    "        if steps > 0:\n",
    "            new_lr = 0.05 * (0.1**steps)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = new_lr\n",
    "            return new_lr\n",
    "        return 0.05\n",
    "\n",
    "    def log(self, lr, epoch, log_dict):\n",
    "        # tensorboard log\n",
    "        for k, v in log_dict.items():\n",
    "            self.tf_writer.add_scalar(k, v, epoch)\n",
    "        self.tf_writer.flush()\n",
    "\n",
    "        # wandb.init(\n",
    "        #     project=\"DTKD\",  # Replace with your project name\n",
    "        #     name=\"DTKD\",      # Optional: Give your run a name\n",
    "        #     config={                     # Optional: Add configuration details\n",
    "        #         \"learning_rate\": 0.05,\n",
    "        #         \"batch_size\": 128,\n",
    "        #         \"epochs\": 3,\n",
    "        #     }\n",
    "        # )\n",
    "        # wandb.log({\"current lr\": lr})\n",
    "        # wandb.log(log_dict)\n",
    "        if log_dict[\"test_acc\"] > self.best_acc:\n",
    "            self.best_acc = log_dict[\"test_acc\"]\n",
    "        #     wandb.run.summary[\"best_acc\"] = self.best_acc\n",
    "        # worklog.txt\n",
    "        with open(os.path.join(self.log_path, \"worklog.txt\"), \"a\") as writer:\n",
    "            lines = [\n",
    "                \"-\" * 25 + os.linesep,\n",
    "                \"epoch: {}\".format(epoch) + os.linesep,\n",
    "                \"lr: {:.2f}\".format(float(lr)) + os.linesep,\n",
    "            ]\n",
    "            for k, v in log_dict.items():\n",
    "                lines.append(\"{}: {:.2f}\".format(k, v) + os.linesep)\n",
    "            lines.append(\"-\" * 25 + os.linesep)\n",
    "            writer.writelines(lines)\n",
    "\n",
    "    def train(self, resume=False, num_epochs=100):\n",
    "        epoch = 1\n",
    "        if resume:\n",
    "            state = load_checkpoint(os.path.join(self.log_path, \"latest\"))\n",
    "            epoch = state[\"epoch\"] + 1\n",
    "            self.distiller.load_state_dict(state[\"model\"])\n",
    "            self.optimizer.load_state_dict(state[\"optimizer\"])\n",
    "            self.best_acc = state[\"best_acc\"]\n",
    "        while epoch < num_epochs + 1:\n",
    "            self.train_epoch(epoch)\n",
    "            epoch += 1\n",
    "        print(log_msg(\"Best accuracy:{}\".format(self.best_acc), \"EVAL\"))\n",
    "        with open(os.path.join(self.log_path, \"worklog.txt\"), \"a\") as writer:\n",
    "            writer.write(\"best_acc\\t\" + \"{:.2f}\".format(float(self.best_acc)))\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        lr = self.adjust_learning_rate(epoch, self.optimizer)\n",
    "        train_meters = {\n",
    "            \"training_time\": AverageMeter(),\n",
    "            \"data_time\": AverageMeter(),\n",
    "            \"losses\": AverageMeter(),\n",
    "            \"top1\": AverageMeter(),\n",
    "            \"top5\": AverageMeter(),\n",
    "        }\n",
    "        num_iter = len(self.train_loader)\n",
    "        pbar = tqdm(range(num_iter))\n",
    "\n",
    "        # train loops\n",
    "        self.distiller.train()\n",
    "        for idx, data in enumerate(self.train_loader):\n",
    "            msg, train_loss = self.train_iter(data, epoch, train_meters)\n",
    "            pbar.set_description(log_msg(msg, \"TRAIN\"))\n",
    "            pbar.update()\n",
    "        pbar.close()\n",
    "\n",
    "        test_acc, test_acc_top5, test_loss = validate(self.val_loader, self.distiller)\n",
    "\n",
    "        dtkd_losses.append({\"train_loss\": train_loss, \"test_loss\": test_loss})\n",
    "        dtkd_accuracies.append({\"acc@1\": test_acc.item(), \"acc@5\": test_acc_top5.item()})\n",
    "        # log\n",
    "        log_dict = OrderedDict(\n",
    "            {\n",
    "                \"train_acc\": train_meters[\"top1\"].avg,\n",
    "                \"train_loss\": train_meters[\"losses\"].avg,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"test_acc_top5\": test_acc_top5,\n",
    "                \"test_loss\": test_loss,\n",
    "            }\n",
    "        )\n",
    "        self.log(lr, epoch, log_dict)\n",
    "        # saving checkpoint\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": self.distiller.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"best_acc\": self.best_acc,\n",
    "        }\n",
    "        student_state = {\"model\": self.distiller.student.state_dict()}\n",
    "        save_checkpoint(state, os.path.join(self.log_path, \"latest\"))\n",
    "        save_checkpoint(\n",
    "            student_state, os.path.join(self.log_path, \"student_latest\")\n",
    "        )\n",
    "        if epoch % 20 == 0:\n",
    "            save_checkpoint(\n",
    "                state, os.path.join(self.log_path, \"epoch_{}\".format(epoch))\n",
    "            )\n",
    "            save_checkpoint(\n",
    "                student_state,\n",
    "                os.path.join(self.log_path, \"student_{}\".format(epoch)),\n",
    "            )\n",
    "        # update the best\n",
    "        if test_acc >= self.best_acc:\n",
    "            save_checkpoint(state, os.path.join(self.log_path, \"best\"))\n",
    "            save_checkpoint(\n",
    "                student_state, os.path.join(self.log_path, \"student_best\")\n",
    "            )\n",
    "\n",
    "    def train_iter(self, data, epoch, train_meters):\n",
    "        self.optimizer.zero_grad()\n",
    "        train_start_time = time.time()\n",
    "        image, target = data  # Adjusted to match the output of your data loader\n",
    "        train_meters[\"data_time\"].update(time.time() - train_start_time)\n",
    "        image = image.float()\n",
    "        image = image.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True)\n",
    "    \n",
    "        # forward\n",
    "        preds, losses_dict = self.distiller(image=image, target=target, epoch=epoch)\n",
    "    \n",
    "        # backward\n",
    "        loss = sum([l.mean() for l in losses_dict.values()])\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        train_meters[\"training_time\"].update(time.time() - train_start_time)\n",
    "        # collect info\n",
    "        batch_size = image.size(0)\n",
    "        acc1, acc5 = accuracy(preds, target, topk=(1, 5))\n",
    "        train_meters[\"losses\"].update(loss.cpu().detach().numpy().mean(), batch_size)\n",
    "        train_meters[\"top1\"].update(acc1[0], batch_size)\n",
    "        train_meters[\"top5\"].update(acc5[0], batch_size)\n",
    "        # print info\n",
    "        msg = \"Epoch:{}| Time(data):{:.3f}| Time(train):{:.3f}| Loss:{:.4f}| Top-1:{:.3f}| Top-5:{:.3f}\".format(\n",
    "            epoch,\n",
    "            train_meters[\"data_time\"].avg,\n",
    "            train_meters[\"training_time\"].avg,\n",
    "            train_meters[\"losses\"].avg,\n",
    "            train_meters[\"top1\"].avg,\n",
    "            train_meters[\"top5\"].avg,\n",
    "        )\n",
    "        return (msg, train_meters[\"losses\"].avg)\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def validate(val_loader, distiller):\n",
    "    batch_time, losses, top1, top5 = [AverageMeter() for _ in range(4)]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    num_iter = len(val_loader)\n",
    "    pbar = tqdm(range(num_iter))\n",
    "\n",
    "    distiller.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for idx, (image, target) in enumerate(val_loader):\n",
    "            image = image.float()\n",
    "            image = image.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "            output = distiller(image=image)\n",
    "            loss = criterion(output, target)\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            batch_size = image.size(0)\n",
    "            losses.update(loss.cpu().detach().numpy().mean(), batch_size)\n",
    "            top1.update(acc1[0], batch_size)\n",
    "            top5.update(acc5[0], batch_size)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "            msg = \"Top-1:{top1.avg:.3f}| Top-5:{top5.avg:.3f}\".format(\n",
    "                top1=top1, top5=top5\n",
    "            )\n",
    "            pbar.set_description(log_msg(msg, \"EVAL\"))\n",
    "            pbar.update()\n",
    "    pbar.close()\n",
    "    return top1.avg, top5.avg, losses.avg\n",
    "\n",
    "def log_msg(msg, mode=\"INFO\"):\n",
    "    color_map = {\n",
    "        \"INFO\": 36,\n",
    "        \"TRAIN\": 32,\n",
    "        \"EVAL\": 31,\n",
    "    }\n",
    "    msg = \"\\033[{}m[{}] {}\\033[0m\".format(color_map[mode], mode, msg)\n",
    "    return msg\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def save_checkpoint(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        torch.save(obj, f)\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return torch.load(f, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:34:58.087761Z",
     "iopub.status.busy": "2025-01-14T14:34:58.087452Z",
     "iopub.status.idle": "2025-01-14T14:34:58.108794Z",
     "shell.execute_reply": "2025-01-14T14:34:58.107872Z",
     "shell.execute_reply.started": "2025-01-14T14:34:58.087735Z"
    },
    "papermill": {
     "duration": 0.023016,
     "end_time": "2024-12-28T15:03:24.769399",
     "exception": false,
     "start_time": "2024-12-28T15:03:24.746383",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LossManager:\n",
    "    def __init__(self, alpha, beta, gamma, initial_temperature):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.current_temperature = initial_temperature\n",
    "        \n",
    "    def cosine_loss(self, student_logits, teacher_logits):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity loss between student and teacher logits.\n",
    "\n",
    "        Args:\n",
    "            student_logits (torch.Tensor): Logits from student model.\n",
    "            teacher_logits (torch.Tensor): Logits from teacher model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Cosine similarity loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Normalize logits\n",
    "        student_norm = F.normalize(student_logits, p=2, dim=1)\n",
    "        teacher_norm = F.normalize(teacher_logits, p=2, dim=1)\n",
    "        \n",
    "        # Compute cosine similarity loss\n",
    "        cosine_loss = 1 - F.cosine_similarity(student_norm, teacher_norm).mean()\n",
    "        return cosine_loss\n",
    "\n",
    "    def rmse_loss(self, student_logits, teacher_logits):\n",
    "        \"\"\"\n",
    "        Compute Root Mean Square Error (RMSE) between student and teacher logits.\n",
    "\n",
    "        Args:\n",
    "            student_logits (torch.Tensor): Logits from student model.\n",
    "            teacher_logits (torch.Tensor): Logits from teacher model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: RMSE loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        rmse = torch.sqrt(F.mse_loss(student_logits, teacher_logits))\n",
    "        return rmse\n",
    "        \n",
    "    def mae_loss(self, student_logits, teacher_logits):\n",
    "        \"\"\"\n",
    "        Compute Root Mean Square Error (RMSE) between student and teacher logits.\n",
    "\n",
    "        Args:\n",
    "            student_logits (torch.Tensor): Logits from student model.\n",
    "            teacher_logits (torch.Tensor): Logits from teacher model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: RMSE loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        rmse = torch.nn.L1Loss()(student_logits, teacher_logits)\n",
    "        return rmse\n",
    "\n",
    "    def hard_loss(self, student_logits, outputs):\n",
    "        \"\"\"\n",
    "        Compute hard loss (cross-entropy) between student logits and true labels.\n",
    "\n",
    "        Args:\n",
    "            student_logits (torch.Tensor): Logits from student model.\n",
    "            outputs (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.nn.CrossEntropyLoss(label_smoothing=0.1)(student_logits, outputs)\n",
    "\n",
    "    def soft_distillation_loss(self, student_logits, teacher_logits):\n",
    "        \"\"\"\n",
    "        Compute knowledge distillation loss with dynamic temperature.\n",
    "\n",
    "        Args:\n",
    "            student_logits (torch.Tensor): Logits from student model.\n",
    "            teacher_logits (torch.Tensor): Logits from teacher model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Knowledge distillation loss.\n",
    "        \"\"\"\n",
    "        soft_targets = F.softmax(teacher_logits / self.current_temperature, dim=1)\n",
    "        soft_predictions = F.log_softmax(student_logits / self.current_temperature, dim=1)\n",
    "        \n",
    "        loss = F.kl_div(soft_predictions, soft_targets, reduction='batchmean')\n",
    "        return loss * (self.current_temperature ** 2)\n",
    "\n",
    "    def combined_loss(self, student_logits, teacher_logits, outputs):\n",
    "        \"\"\"Only include the additional losses (cosine and RMSE) here\"\"\"\n",
    "        # Cosine loss\n",
    "        cosine_loss = self.beta * self.cosine_loss(student_logits, teacher_logits)\n",
    "        # RMSE loss\n",
    "        rmse_loss = self.gamma * self.rmse_loss(student_logits, teacher_logits)\n",
    "        return cosine_loss + rmse_loss\n",
    "    \n",
    "\n",
    "class DynamicTemperatureScheduler(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic Temperature Scheduler for Knowledge Distillation.\n",
    "\n",
    "    Args:\n",
    "        initial_temperature (float): Starting temperature value.\n",
    "        min_temperature (float): Minimum allowable temperature.\n",
    "        max_temperature (float): Maximum allowable temperature.\n",
    "        schedule_type (str): Type of temperature scheduling strategy.\n",
    "        loss_type (str): Type of loss to use (combined or general KD).\n",
    "        alpha (float): Importance for soft loss, 1-alpha for hard loss.\n",
    "        beta (float): Importance of cosine loss.\n",
    "        gamma (float): Importance for RMSE loss.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        initial_temperature=4.0, \n",
    "        min_temperature=1.0, \n",
    "        max_temperature=8,\n",
    "        max_epoch=50,\n",
    "        warmup=20,\n",
    "        alpha=0.5,\n",
    "        beta=0.9,\n",
    "        gamma=0.5,\n",
    "    ):\n",
    "        super(DynamicTemperatureScheduler, self).__init__()\n",
    "\n",
    "        self.current_temperature = initial_temperature\n",
    "        self.initial_temperature = initial_temperature\n",
    "        self.min_temperature = min_temperature\n",
    "        self.max_temperature = max_temperature\n",
    "        self.max_epoch = max_epoch\n",
    "        self.warmup = warmup\n",
    "        \n",
    "        # Tracking training dynamics\n",
    "        self.loss_history = []\n",
    "        self.student_loss = []\n",
    "\n",
    "        # Constants for importance\n",
    "        self.loss_manager = LossManager(alpha, beta, gamma, initial_temperature)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def update_temperature(self, current_epoch, teacher_loss, student_loss):\n",
    "        \"\"\"\n",
    "        Dynamically update temperature based on training dynamics.\n",
    "\n",
    "        Args:\n",
    "            current_epoch (int): Current training epoch.\n",
    "            total_epochs (int): Total number of training epochs.\n",
    "            teacher_loss (float): Loss of teacher model.\n",
    "            student_loss (float): Loss of student model.\n",
    "        \"\"\"\n",
    "        self.loss_history.append((teacher_loss, student_loss))\n",
    "        # Cosine annealing with adaptive scaling\n",
    "        total_epochs = self.max_epoch\n",
    "        progress = current_epoch / total_epochs\n",
    "        scale_factor = 0.5 + torch.cos(\n",
    "            torch.pi * torch.tensor(\n",
    "                progress * 0.7, \n",
    "                device=\"cuda\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        window_size = 5\n",
    "        self.loss_history = self.loss_history[-window_size:]\n",
    "        recent_losses = self.loss_history\n",
    "        teacher_losses = torch.tensor([loss[0] for loss in recent_losses], device='cuda')\n",
    "        student_losses = torch.tensor([loss[1] for loss in recent_losses], device='cuda')\n",
    "\n",
    "        teacher_loss = teacher_losses.mean()\n",
    "        student_loss = student_losses.mean()\n",
    "        \n",
    "        # Dynamic scaling based on loss divergence\n",
    "        loss_divergence = torch.abs(teacher_loss - student_loss)\n",
    "        adaptive_scale = 0.8 + torch.log(1 + loss_divergence)\n",
    "        \n",
    "        # Update temperature\n",
    "        self.current_temperature = max(\n",
    "            self.min_temperature, \n",
    "            min(\n",
    "                self.max_temperature*torch.exp(\n",
    "                    torch.tensor(\n",
    "                        -progress*.4, \n",
    "                        device=\"cuda\"\n",
    "                    )\n",
    "                ), \n",
    "                0.1 + self.initial_temperature * scale_factor * adaptive_scale * torch.exp(\n",
    "                    torch.tensor(\n",
    "                        -progress*.75, device=\"cuda\"\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.loss_manager.current_temperature = self.current_temperature\n",
    "        \n",
    "    def get_temperature(self):\n",
    "        \"\"\"\n",
    "        Retrieve current temperature value.\n",
    "\n",
    "        Returns:\n",
    "            float: Current dynamic temperature.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.current_temperature\n",
    "        \n",
    "    def forward(self, student_logits, teacher_logits, outputs):\n",
    "        \"\"\"\n",
    "        Forward pass to compute the loss based on the specified loss type.\n",
    "\n",
    "        Args:\n",
    "            student_logits (torch.Tensor): Logits from student model.\n",
    "            teacher_logits (torch.Tensor): Logits from teacher model.\n",
    "            outputs (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.current_temperature > 1.5:\n",
    "            return min(self.max_epoch / self.warmup, 1.0) * self.loss_manager.combined_loss(\n",
    "                student_logits, \n",
    "                teacher_logits, \n",
    "                outputs\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # Soft loss\n",
    "            soft_loss = 0.15 * self.loss_manager.soft_distillation_loss(student_logits, teacher_logits)\n",
    "            # Hard loss\n",
    "            hard_loss = (0.85) * self.loss_manager.hard_loss(student_logits, outputs)\n",
    "            \n",
    "            return soft_loss + hard_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:34:58.111395Z",
     "iopub.status.busy": "2025-01-14T14:34:58.111075Z",
     "iopub.status.idle": "2025-01-14T14:34:58.130376Z",
     "shell.execute_reply": "2025-01-14T14:34:58.129352Z",
     "shell.execute_reply.started": "2025-01-14T14:34:58.111370Z"
    },
    "papermill": {
     "duration": 0.020375,
     "end_time": "2024-12-28T15:03:24.792018",
     "exception": false,
     "start_time": "2024-12-28T15:03:24.771643",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy(outputs, targets, topk=(1, 5)):\n",
    "    \"\"\"\n",
    "    Calculate top-k accuracy\n",
    "    \n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model predictions\n",
    "        targets (torch.Tensor): Ground truth labels\n",
    "        topk (tuple): Top-k values to compute accuracy\n",
    "    \n",
    "    Returns:\n",
    "        list: Top-k accuracies\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = targets.size(0)\n",
    "\n",
    "        # Get top-k predictions\n",
    "        _, pred = outputs.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "\n",
    "        # Calculate accuracies\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        \n",
    "        return res\n",
    "        \n",
    "def adjust_learning_rate(epoch, lr, optimizer):\n",
    "    steps = np.sum(epoch > np.asarray([33, 60, 80, 95]))\n",
    "    lrs = [0.0015, 0.0003, 0.00015, 1e-6]\n",
    "    print(steps)\n",
    "    if steps > 0:\n",
    "        new_lr = lrs[steps-1]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = new_lr\n",
    "        print(\"Learning rate is set to {}\".format(new_lr))\n",
    "        return new_lr\n",
    "    return lr\n",
    "\n",
    "def train_knowledge_distillation(\n",
    "    name,\n",
    "    teacher_model, \n",
    "    student_model, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    lr,\n",
    "    epochs=50, \n",
    "    val_steps=10,\n",
    "    temperature_scheduler=None,\n",
    "    scheduler=None,\n",
    "    save_path=\"./output/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train student model with periodic validation\n",
    "    \n",
    "    Args:\n",
    "        teacher_model (nn.Module): Pre-trained teacher model\n",
    "        student_model (nn.Module): Model to be distilled\n",
    "        train_dataset (Dataset): Training data\n",
    "        val_dataset (Dataset): Validation data\n",
    "        epochs (int): Total training epochs\n",
    "        alpha (float): Loss balancing coefficient\n",
    "        temperature_scheduler (DynamicTemperatureScheduler): Temperature scheduler\n",
    "        save_path (str): Path to save the best model\n",
    "    \"\"\"\n",
    "    \n",
    "    run = wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"DTAD_Trials\",\n",
    "        name=name\n",
    "    )\n",
    "    \n",
    "    # Optimizer and criterion\n",
    "    student_optimizer = optimizer\n",
    "    task_criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # Set models to appropriate modes\n",
    "    teacher_model.eval()\n",
    "    val_loss = 0\n",
    "    top1_acc = 0\n",
    "    top5_acc = 0\n",
    "    \n",
    "    print(\"-\" * 15 + \" Teacher Validation \" + \"-\" * 15)\n",
    "    with torch.no_grad():\n",
    "        for val_x, val_y in val_loader:\n",
    "            val_x, val_y = val_x.to(\"cuda\"), val_y.to(\"cuda\")\n",
    "            val_outputs = teacher_model(val_x)\n",
    "            val_batch_loss = task_criterion(val_outputs, val_y)\n",
    "            val_loss += val_batch_loss.item()\n",
    "            \n",
    "            # Calculate accuracies\n",
    "            batch_top1, batch_top5 = calculate_accuracy(val_outputs, val_y)\n",
    "            top1_acc += batch_top1.item()\n",
    "            top5_acc += batch_top5.item()\n",
    "    \n",
    "    # Average validation metrics\n",
    "    val_loss /= len(val_loader)\n",
    "    top1_acc /= len(val_loader)\n",
    "    top5_acc /= len(val_loader)\n",
    "    \n",
    "    print(f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Top-1 Accuracy: {top1_acc:.2f}% | Top-5 Accuracy: {top5_acc:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    best_top1_acc = 0.0  # Initialize best accuracy tracker\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        print(\"-\" * 16 + \" Training \" + \"-\" * 16)\n",
    "        student_model.train()\n",
    "        train_loss = 0\n",
    "        train_acc_1 = 0\n",
    "        train_acc_5 = 0\n",
    "\n",
    "        lr = adjust_learning_rate(epoch+1, lr, student_optimizer) if scheduler == None else 0\n",
    "\n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            batch_x, batch_y = batch_x.to('cuda'), batch_y.to('cuda')\n",
    "            \n",
    "            # Forward passes\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(batch_x)\n",
    "            \n",
    "            student_logits = student_model(batch_x)\n",
    "            \n",
    "            # Knowledge distillation loss\n",
    "            if temperature_scheduler:\n",
    "                # Combine losses\n",
    "                total_batch_loss = temperature_scheduler(\n",
    "                    epoch,\n",
    "                    student_logits=student_logits,\n",
    "                    teacher_logits=teacher_logits,\n",
    "                    outputs=batch_y\n",
    "                )\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                student_optimizer.zero_grad()\n",
    "                total_batch_loss.backward()\n",
    "                student_optimizer.step()\n",
    "            \n",
    "            # Calculate accuracies\n",
    "            acc1, acc5 = calculate_accuracy(student_logits, batch_y) \n",
    "            train_loss += total_batch_loss.item()\n",
    "            train_acc_1 += acc1.item()\n",
    "            train_acc_5 += acc5.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1} | Batch {batch_idx}/{len(train_loader)} | \"\n",
    "                      f\"Loss: {total_batch_loss.item():.4f} | Temp: {temperature_scheduler.get_temperature():.2f} | \"\n",
    "                      f\"Acc@1: {acc1.item():.2f}% | Acc@5: {acc5.item():.2f}%\")\n",
    "        \n",
    "        # Epoch-end metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc_1 /= len(train_loader)\n",
    "        train_acc_5 /= len(train_loader)\n",
    "        \n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {train_loss:.4f} | \"\n",
    "              f\"Acc@1: {train_acc_1:.2f}% | Acc@5: {train_acc_5:.2f}%\")\n",
    "        print(\"-\" * 42)\n",
    "\n",
    "    # if (epoch+1) % val_steps == 0:    \n",
    "        # Validation phase\n",
    "        print(\"-\" * 15 + \" Validation \" + \"-\" * 15)\n",
    "        student_model.eval()\n",
    "        val_loss = 0\n",
    "        top1_acc = 0\n",
    "        top5_acc = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_loader:\n",
    "                val_x, val_y = val_x.to(\"cuda\"), val_y.to(\"cuda\")\n",
    "                val_outputs = student_model(val_x)\n",
    "                val_batch_loss = task_criterion(val_outputs, val_y)\n",
    "                val_loss += val_batch_loss.item()\n",
    "                \n",
    "                # Calculate accuracies\n",
    "                batch_top1, batch_top5 = calculate_accuracy(val_outputs, val_y)\n",
    "                top1_acc += batch_top1.item()\n",
    "                top5_acc += batch_top5.item()\n",
    "        \n",
    "        # Average validation metrics\n",
    "        val_loss /= len(val_loader)\n",
    "        top1_acc /= len(val_loader)\n",
    "        top5_acc /= len(val_loader)\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train_acc\": train_acc_1, \n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_acc\": top1_acc, \n",
    "                \"val_loss\": val_loss,\n",
    "                \"lr\": lr,\n",
    "                \"temp\": temperature_scheduler.get_temperature()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        our_losses.append({\"train_loss\": train_loss, \"test_loss\": val_loss})\n",
    "        our_accuracies.append({\"acc@1\": top1_acc, \"acc@5\": top5_acc})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Top-1 Accuracy: {top1_acc:.2f}% | Top-5 Accuracy: {top5_acc:.2f}%\")\n",
    "        print(\"-\" * 42)\n",
    "        \n",
    "        # Save the best model\n",
    "        if top1_acc > best_top1_acc:\n",
    "            best_top1_acc = top1_acc\n",
    "            torch.save(student_model.state_dict(), f\"DTAD_@{top1_acc}.pth\")\n",
    "            print(f\"Best model saved at epoch {epoch+1} with Top-1 Accuracy: {best_top1_acc:.2f}%\")\n",
    "    print(\"Best Model Accuracy: \", best_top1_acc)\n",
    "    run.finish()\n",
    "    \n",
    "    torch.save(student_model.state_dict(), \"trained_studentDTAD.pth\")\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:34:58.132433Z",
     "iopub.status.busy": "2025-01-14T14:34:58.132072Z",
     "iopub.status.idle": "2025-01-14T14:35:04.684034Z",
     "shell.execute_reply": "2025-01-14T14:35:04.683267Z",
     "shell.execute_reply.started": "2025-01-14T14:34:58.132406Z"
    },
    "papermill": {
     "duration": 7219.361104,
     "end_time": "2024-12-28T17:03:44.155369",
     "exception": false,
     "start_time": "2024-12-28T15:03:24.794265",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# CIFAR-10 Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # Mean and std of CIFAR-10\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR100(root=\"./data\", train=True, transform=transform, download=True)\n",
    "val_dataset = torchvision.datasets.CIFAR100(root=\"./data\", train=False, transform=val_transform, download=True)\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "batch_size=128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "\n",
    "teacher_model, path = cifar_model_dict[\"resnet32x4\"]\n",
    "teacher_model = teacher_model(num_classes=num_classes)\n",
    "teacher_model.load_state_dict(torch.load(path)[\"model\"])\n",
    "teacher_model.to(\"cuda\")\n",
    "\n",
    "student_model, path = cifar_model_dict[\"resnet8x4\"]\n",
    "student_model = student_model(num_classes=num_classes)\n",
    "student_model.to(\"cuda\")\n",
    "print(\"models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:35:05.652362Z",
     "iopub.status.busy": "2025-01-14T14:35:05.652092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_epoch = 100\n",
    "lr = 3e-2\n",
    "\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=3e-2, weight_decay=5e-4, eps=0.1)\n",
    "temp_scheduler = DynamicTemperatureScheduler(alpha=0.5, max_epoch=max_epoch, warmup=5)\n",
    "\n",
    "trained_student = train_knowledge_distillation(\n",
    "    \"32x4 to 8x4 (Ours) adamW+warmup+CosineAnneal.\",\n",
    "    teacher_model, \n",
    "    student_model, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    batch_size,\n",
    "    optimizer,\n",
    "    lr=lr,\n",
    "    epochs=max_epoch,\n",
    "    val_steps=1,\n",
    "    temperature_scheduler=temp_scheduler,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# student_model, path = cifar_model_dict[\"resnet8x4\"]\n",
    "# student_model = student_model(num_classes=num_classes)\n",
    "# student_model.to(\"cuda\")\n",
    "\n",
    "# distiller = DTKD(student_model, teacher_model)\n",
    "\n",
    "# # # Initialize the CRDTrainer\n",
    "# trainer = BaseTrainer(\n",
    "#     experiment_name=\"DTKD\",\n",
    "#     distiller=distiller,\n",
    "#     train_loader=train_loader, \n",
    "#     val_loader=val_loader\n",
    "# )\n",
    "\n",
    "# trainer.train(num_epochs=max_epoch)\n",
    "\n",
    "# print(f\"DTKD lossess: {dtkd_losses}\")\n",
    "# print(f\"Our lossess: {our_losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_losses():\n",
    "    # Extracting train and test losses for plotting\n",
    "    dtkd_train_loss = [entry['train_loss'] for entry in dtkd_losses]\n",
    "    dtkd_test_loss = [entry['test_loss'] for entry in dtkd_losses]\n",
    "    our_train_loss = [entry['train_loss'] for entry in our_losses]\n",
    "    our_test_loss = [entry['test_loss'] for entry in our_losses]\n",
    "    \n",
    "    # FOR 100 EPOCH\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 10)) \n",
    "    \n",
    "    # Train Losses\n",
    "    plt.subplot(2, 1, 1)  # Positioning in the first row\n",
    "    plt.plot(dtkd_train_loss, label=\"DTKD Train Loss\", color='blue')\n",
    "    plt.plot(our_train_loss, label=\"Our Train Loss\", color='red')\n",
    "    plt.title(\"Train Losses\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    # Test Losses\n",
    "    plt.subplot(2, 1, 2)  # Positioning in the second row\n",
    "    plt.plot(dtkd_test_loss, label=\"DTKD Test Loss\", color='blue')\n",
    "    plt.plot(our_test_loss, label=\"Our Test Loss\", color='red')\n",
    "    plt.title(\"Test Losses\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "    plt.show()\n",
    "\n",
    "plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(f\"DTKD accuracies: {dtkd_accuracies}\")\n",
    "print(f\"Our accuracies: {our_accuracies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracies():\n",
    "    \n",
    "    # # Extract data\n",
    "    # dtkd_acc1 = [entry['acc@1'] for entry in dtkd_accuracies]\n",
    "    # dtkd_acc5 = [entry['acc@5'] for entry in dtkd_accuracies]\n",
    "    our_acc1 = [entry['acc@1'] for entry in our_accuracies]\n",
    "    our_acc5 = [entry['acc@5'] for entry in our_accuracies]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # plt.plot(epochs, dtkd_acc1, label=\"DTKD acc@1\", color='blue')\n",
    "    # plt.plot(epochs, dtkd_acc5, label=\"DTKD acc@5\", color='cyan')\n",
    "    plt.plot(our_acc1, label=\"Our acc@1\", color='red')\n",
    "    plt.plot(our_acc5, label=\"Our acc@5\", color='blue')\n",
    "    \n",
    "    # Graph details\n",
    "    plt.title(\"Accuracy Comparison\", fontsize=16)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "    plt.xticks(epochs)  # Show exact epochs on x-axis\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "plot_accuracies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "resnet101-34-kd",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelId": 209416,
     "modelInstanceId": 187338,
     "sourceId": 219664,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7227.877548,
   "end_time": "2024-12-28T17:03:45.676396",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-28T15:03:17.798848",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
